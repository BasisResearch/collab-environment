{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a20766",
   "metadata": {},
   "source": [
    "# Full Tracker: Download, Process, and Upload Data\n",
    "This notebook demonstrates the full pipeline for handling raw data:\n",
    "1. Download data from a cloud bucket.\n",
    "2. Process the data (e.g., align videos, run detection, and tracking).\n",
    "3. Upload the processed data back to the cloud bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d07572",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'collab_env'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Import Utility Functions\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollab_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m expand_path, get_project_root\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollab_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgcs_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GCSClient\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Import Custom Scripts\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'collab_env'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import Utility Functions\n",
    "from collab_env.data.file_utils import expand_path, get_project_root\n",
    "from collab_env.data.gcs_utils import GCSClient\n",
    "\n",
    "# Import Custom Scripts\n",
    "from collab_env.tracking.alignment_gui import align_videos\n",
    "from collab_env.tracking.model.local_model_inference import process_video_with_rfdetr\n",
    "from collab_env.tracking.model.local_model_tracking import (\n",
    "    overlay_tracks_on_video,\n",
    "    run_tracking,\n",
    "    visualize_detections_from_video,\n",
    ")\n",
    "from collab_env.tracking.thermal_processing import (\n",
    "    process_directory,\n",
    "    validate_session_structure,\n",
    ")\n",
    "\n",
    "skip_download = True\n",
    "skip_thermal_extraction = True\n",
    "\n",
    "# Reload helper for dev work\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Configuration\n",
    "load_dotenv()\n",
    "data_key = expand_path(os.environ.get(\"COLLAB_DATA_KEY\", \"\"), get_project_root())\n",
    "PROJECT_ID = \"collab-data-463313\"\n",
    "\n",
    "CREDENTIALS_PATH = expand_path(data_key.as_posix(), get_project_root())\n",
    "\n",
    "# Connect to Google Cloud Storage\n",
    "gcs_client = GCSClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    credentials_path=CREDENTIALS_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify connection\n",
    "print(\"Available buckets:\", gcs_client.list_buckets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b948f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"fieldwork_curated\"  # Update with your bucket name\n",
    "gcs_client.glob(f\"{BUCKET_NAME}/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0891ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data from Cloud Bucket\n",
    "SESSION_FOLDER = \"2024_02_06-session_0001\"\n",
    "CLOUD_PREFIX = f\"{BUCKET_NAME}/{SESSION_FOLDER}\"  # Update with data folder (session)\n",
    "gcs_client.glob(f\"{CLOUD_PREFIX}/**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4e7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DOWNLOAD_DIR = expand_path(f\"data/raw/{SESSION_FOLDER}\", get_project_root())\n",
    "LOCAL_PROCESSED_DIR = expand_path(\n",
    "    f\"data/processed/{SESSION_FOLDER}\", get_project_root()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_download:\n",
    "    if not LOCAL_DOWNLOAD_DIR.exists():\n",
    "        LOCAL_DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    if not LOCAL_PROCESSED_DIR.exists():\n",
    "        LOCAL_PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for blob in gcs_client.glob(f\"{CLOUD_PREFIX}/**\"):\n",
    "        relative_path = Path(blob).relative_to(f\"{CLOUD_PREFIX}\")\n",
    "        local_name = relative_path.name\n",
    "        suffix = relative_path.suffix\n",
    "        print(f\"local_name: {local_name}, suffix: {suffix}\")\n",
    "        if len(str(suffix)) > 0:\n",
    "            # print(\"File!\")\n",
    "            parent_folder = relative_path.parent\n",
    "            if not Path(LOCAL_DOWNLOAD_DIR / parent_folder).exists():\n",
    "                print(f\"Creating folder: {LOCAL_DOWNLOAD_DIR / parent_folder}\")\n",
    "                Path(LOCAL_DOWNLOAD_DIR / parent_folder).mkdir(\n",
    "                    parents=True, exist_ok=True\n",
    "                )\n",
    "            # print(f\"parent_folder: {parent_folder}\")\n",
    "            local_path = LOCAL_DOWNLOAD_DIR / parent_folder / local_name\n",
    "            print(f\"Downloading file: {blob} to {local_path}\")\n",
    "            gcs_client.gcs.get_file(blob, str(local_path))\n",
    "        else:\n",
    "\n",
    "            if not Path(LOCAL_PROCESSED_DIR / relative_path).exists():\n",
    "                print(f\"Creating folder: {LOCAL_PROCESSED_DIR / relative_path}\")\n",
    "                Path(LOCAL_PROCESSED_DIR / relative_path).mkdir(\n",
    "                    parents=True, exist_ok=True\n",
    "                )\n",
    "        # check if there is an extension, if not this is a folder and we need to create it\n",
    "\n",
    "    # print(\"Downloaded files:\", list(LOCAL_DOWNLOAD_DIR.iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e8dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate session structure\n",
    "print(\"Validating session structure...\")\n",
    "issues = validate_session_structure(LOCAL_DOWNLOAD_DIR)\n",
    "print(f\"Issues found: {issues if len(issues)>0 else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b804e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_thermal_extraction:\n",
    "    # thermal files processing\n",
    "    print(\"Processing thermal files...\")\n",
    "\n",
    "    # call with preview=False to choose the vmin/vmax automatically, otherwise the user will be asked to choose the vmin/vmax\n",
    "    # process_directory(folder_path=LOCAL_DOWNLOAD_DIR, out_path=LOCAL_DOWNLOAD_DIR, color='magma', preview=True, max_frames=None, fps=30)\n",
    "    process_directory(\n",
    "        folder_path=LOCAL_DOWNLOAD_DIR,\n",
    "        out_path=LOCAL_PROCESSED_DIR,\n",
    "        color=\"magma\",\n",
    "        preview=True,\n",
    "        max_frames=100,\n",
    "        fps=30,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd97256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default parameters for alignment\n",
    "\n",
    "frame_size = (640, 480)  # Default frame size\n",
    "max_frames = 10  # Process all frames by default\n",
    "warp_to = \"rgb\"  # Default warp to rgb, thermal is changing, not rgb\n",
    "rotation_angle = 0.0  # Default rotation angle\n",
    "skip_homography = False  # Default to not skip homography\n",
    "skip_translation = True  # Default to skip translation\n",
    "camera_numbers = [1, 2]\n",
    "\n",
    "for camera in camera_numbers:\n",
    "    print(f\"Processing camera {camera}...\")\n",
    "\n",
    "    # Dynamically find the RGB and thermal MP4 files\n",
    "    rgb_dir = LOCAL_DOWNLOAD_DIR / f\"rgb_{camera}\"\n",
    "    thermal_dir = LOCAL_PROCESSED_DIR / f\"thermal_{camera}\"\n",
    "\n",
    "    # Find the MP4 file in the RGB directory\n",
    "    rgb_video_files = list(rgb_dir.glob(\"*.MP4\")) + list(rgb_dir.glob(\"*.mp4\"))\n",
    "    print(\"files in rgb_dir:\", rgb_video_files)\n",
    "    if len(rgb_video_files) == 0:\n",
    "        print(f\"No MP4 file found in {rgb_dir}. Skipping camera {camera}.\")\n",
    "        continue\n",
    "    elif len(rgb_video_files) > 1:\n",
    "        print(f\"Multiple MP4 files found in {rgb_dir}. Using the first one.\")\n",
    "    rgb_video_path = rgb_video_files[0]\n",
    "\n",
    "    # Find the MP4 file in the thermal directory\n",
    "    thermal_video_files = list(thermal_dir.glob(\"*.mp4\")) + list(\n",
    "        thermal_dir.glob(\"*.MP4\")\n",
    "    )\n",
    "    print(\"files in thermal_dir:\", thermal_video_files)\n",
    "    if len(thermal_video_files) == 0:\n",
    "        print(f\"No MP4 file found in {thermal_dir}. Skipping camera {camera}.\")\n",
    "        continue\n",
    "    elif len(thermal_video_files) > 1:\n",
    "        print(f\"Multiple MP4 files found in {thermal_dir}. Using the first one.\")\n",
    "    thermal_video_path = thermal_video_files[0]\n",
    "\n",
    "    print(f\"RGB video path: {rgb_video_path}\")\n",
    "    print(f\"Thermal video path: {thermal_video_path}\")\n",
    "\n",
    "    output_dir_rgb = LOCAL_PROCESSED_DIR / \"aligned\" / f\"rgb_{camera}\"\n",
    "    output_dir_thm = LOCAL_PROCESSED_DIR / \"aligned\" / f\"thermal_{camera}\"\n",
    "    output_dir_rgb.mkdir(parents=True, exist_ok=True)\n",
    "    output_dir_thm.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Align videos\n",
    "    print(f\"Aligning videos for camera {camera}...\")\n",
    "\n",
    "    align_videos(\n",
    "        rgb_video_path,\n",
    "        thermal_video_path,\n",
    "        output_dir_rgb,\n",
    "        output_dir_thm,\n",
    "        frame_size=frame_size,\n",
    "        max_frames=max_frames,\n",
    "        warp_to=warp_to,\n",
    "        rotation_angle=rotation_angle,\n",
    "        skip_homography=skip_homography,\n",
    "        skip_translation=skip_translation,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detection-tracking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection and tracking\n",
    "print(\"Running detection and tracking...\")\n",
    "for camera in camera_numbers:\n",
    "    print(f\"Running detection and tracking on: thermal_{camera}\")\n",
    "\n",
    "    # Define paths for the thermal video and model inference\n",
    "    thermal_video_path = (\n",
    "        LOCAL_PROCESSED_DIR\n",
    "        / \"aligned\"\n",
    "        / f\"thermal_{camera}\"\n",
    "        / f\"warped_thermal_{camera}.mp4\"\n",
    "    )\n",
    "    if not thermal_video_path.exists():\n",
    "        print(f\"Thermal video not found for camera {camera}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Run local_model_inference script\n",
    "    try:\n",
    "        output_csv_path = (\n",
    "            LOCAL_PROCESSED_DIR / \"aligned\" / f\"thermal_{camera}\" / \"output_results.csv\"\n",
    "        )\n",
    "        output_video_path = (\n",
    "            LOCAL_PROCESSED_DIR\n",
    "            / \"aligned\"\n",
    "            / f\"thermal_{camera}\"\n",
    "            / \"annotated_warped_thermal.mp4\"\n",
    "        )\n",
    "        checkpoint_path = \"scripts/model/weights.pt\"\n",
    "\n",
    "        process_video_with_rfdetr(\n",
    "            video_path=thermal_video_path,\n",
    "            output_csv_path=output_csv_path,\n",
    "            output_video_path=output_video_path,\n",
    "            checkpoint_path=checkpoint_path,\n",
    "            confidence=0.5,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during object detection for camera {camera}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Run tracking\n",
    "    print(f\"Running tracking on: thermal_{camera}\")\n",
    "    run_tracking(LOCAL_PROCESSED_DIR, \"thermal\", camera)\n",
    "\n",
    "    tracked_csv = (\n",
    "        LOCAL_PROCESSED_DIR\n",
    "        / \"aligned\"\n",
    "        / f\"thermal_{camera}\"\n",
    "        / f\"thermal_{camera}_tracks.csv\"\n",
    "    )\n",
    "    if not tracked_csv.exists():\n",
    "        print(f\"Tracking CSV not found for camera {camera}. Skipping visualization.\")\n",
    "        continue\n",
    "\n",
    "    # Visualization\n",
    "    visualize_detections_from_video(\n",
    "        csv_path=tracked_csv,\n",
    "        video_path=thermal_video_path,\n",
    "        output_video_path=LOCAL_PROCESSED_DIR\n",
    "        / \"aligned\"\n",
    "        / f\"thermal_{camera}\"\n",
    "        / f\"visualized_thermal_{camera}.mp4\",\n",
    "    )\n",
    "    print(f\"Visualizing tracks for rgb camera {camera}...\")\n",
    "    overlay_tracks_on_video(\n",
    "        csv_path=tracked_csv,\n",
    "        frame_dir=LOCAL_PROCESSED_DIR\n",
    "        / \"aligned\"\n",
    "        / f\"rgb_{camera}\"\n",
    "        / \"annotated_frames\",\n",
    "        output_video=LOCAL_PROCESSED_DIR\n",
    "        / \"aligned\"\n",
    "        / f\"rgb_{camera}\"\n",
    "        / f\"overlayed_tracks_{camera}.mp4\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146dc8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Processed Data to Cloud Bucket\n",
    "CLOUD_PROCESSED_PREFIX = (\n",
    "    \"your-cloud-processed-prefix\"  # Update with your processed data prefix\n",
    ")\n",
    "for file in LOCAL_PROCESSED_DIR.iterdir():\n",
    "    cloud_path = f\"{BUCKET_NAME}/{CLOUD_PROCESSED_PREFIX}/{file.name}\"\n",
    "    gcs_client.upload_file(str(file), cloud_path)\n",
    "\n",
    "print(\"Uploaded processed files:\", list(LOCAL_PROCESSED_DIR.iterdir()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collab-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
