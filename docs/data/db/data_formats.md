# Data Formats Documentation

This document describes the three primary data formats used in the collab-environment project for storing and processing timeseries tracking data.

## Table of Contents

1. [3D Boids Simulations](#1-3d-boids-simulations)
2. [2D Boids Simulations](#2-2d-boids-simulations)
3. [Real-World Tracked Data](#3-real-world-tracked-data)
4. [Summary Comparison](#4-summary-comparison)

---

## 1. 3D Boids Simulations

### Overview
3D boid simulations generated by `collab_env.sim.boids` and stored in the `simulated_data/hackathon/` directory. Each simulation run contains multiple episodes stored as parquet files, with a YAML configuration file describing the simulation parameters.

### Location
- **Data Directory**: `simulated_data/hackathon/`
- **Naming Pattern**: `hackathon-{variant}-{size}-{agents}-{behavior}_sim_run-started-{timestamp}/`
- **Visualization**: `collab_env.dashboard.persistent_video_server --mode simulation`

### Data Structure

#### Session/Run Organization
```
hackathon-boid-small-200-align-cohesion_sim_run-started-20250926-214330/
├── config.yaml                              # Simulation configuration
├── episode-0-completed-20250926-214410.parquet
├── episode-1-completed-20250926-214449.parquet
├── ...
└── episode-9-completed-20250926-215048.parquet
```

#### Parquet File Schema

**File Format**: Apache Parquet (columnar storage)

**Columns** (14 total):

| Column Name | Data Type | Description | Example |
|------------|-----------|-------------|---------|
| `id` | int64 | Unique agent identifier within episode | 1, 2, 3, ... |
| `type` | object (string) | Agent type category | "agent", "target" |
| `time` | int64 | Frame number / timestep | 0, 1, 2, ..., 2999 |
| `x` | float64 | X position in 3D space | 750.5, 848.8, ... |
| `y` | float64 | Y position in 3D space (height) | 20.0, 176.6, ... |
| `z` | float64 | Z position in 3D space | 750.0, 639.1, ... |
| `v_x` | float64 | X velocity component | 0.123, -0.456, ... |
| `v_y` | float64 | Y velocity component | 0.012, 0.034, ... |
| `v_z` | float64 | Z velocity component | -0.234, 0.567, ... |
| `distance_target_center_1` | float64 | Distance to target 1 center | 125.34, 234.56, ... |
| `distance_to_target_mesh_closest_point_1` | float64 | Distance to nearest point on target mesh 1 | 45.67, 89.12, ... |
| `target_mesh_closest_point_1` | object (list) | Closest point coordinates on target mesh | "[848.8, 176.6, 639.1]" |
| `mesh_scene_distance` | float64 | Distance to scene mesh (environment boundary) | 89.83, 58.21, ... |
| `mesh_scene_closest_point` | object (list) | Closest point on scene mesh | "[848.8, 176.6, 639.1]" |

**Data Characteristics**:
- Typical episode: 3000 frames × 30-400 agents = 90,000-1,200,000 rows
- Frame rate: 30 fps (configurable)
- Coordinate system: Right-handed 3D, units typically in scene units (configurable scale)
- Time is discrete (integer frame numbers), not datetime

#### Configuration File Schema (config.yaml)

**File Format**: YAML

**Top-level Sections**:

1. **simulator**: Core simulation parameters
   ```yaml
   simulator:
     seed: [1250, 439, ...]           # Random seeds per episode
     walking: false                    # Ground-based vs flying
     num_agents: 30                    # Number of agents
     num_targets: 1                    # Number of target objects
     num_episodes: 10                  # Episodes to generate
     num_frames: 3000                  # Frames per episode
     target_creation_time: [500]       # When targets appear
     run_sub_folder_prefix: "hackathon-boid-small-200-align-cohesion_sim_run"
   ```

2. **agent**: Agent behavior parameters
   ```yaml
   agent:
     min_separation: 20.0              # Minimum distance between agents
     neighborhood_dist: 80.0           # Vision range
     separation_weight: 0.0            # Avoidance strength
     alignment_weight: 1.0             # Flocking alignment
     cohesion_weight: 0.0              # Attraction to neighbors
     target_weight: [0.002, -1.0, 0.0, 0.0]  # Attraction to targets
     max_speed: 1.0                    # Speed limits
     min_speed: 0.1
     max_force: 0.1                    # Steering force limit
     agent_variants:                   # Multiple agent types
       - type: boid
         num_agents_of_type: 30
         alignment_weight: 1.0
         cohesion_weight: 0.5
   ```

3. **environment**: Scene configuration
   ```yaml
   environment:
     box_size: 1500                    # Bounding box size
     scene_scale: 300.0                # Coordinate scale
     scene_position: [750, 20, 750]    # Scene origin
     init_range_low: 0.4               # Initial position range (normalized)
     init_range_high: 0.6
     height_init_max: 450              # Initial height range
   ```

4. **meshes**: 3D mesh references
   ```yaml
   meshes:
     mesh_scene: 'meshes/Open3dTSDFfusion_mesh.ply'
     scene_angle: [-90.0, 0.0, 0.0]    # Rotation (degrees)
     sub_mesh_target: ['meshes/labeled_meshes/query-tree_top-cluster.ply']
   ```

5. **visuals**: Rendering configuration
   ```yaml
   visuals:
     store_video: false
     width: 1920
     height: 1280
     agent_scale: 2.0
     target_scale: 10.0
   ```

6. **tracks**: Trajectory visualization
   ```yaml
   tracks:
     color_by_time: true
     number_of_color_groups: 1000
   ```

**Path Resolution**: Mesh paths are relative to project root and resolved via `collab_env.data.file_utils.expand_path()`

### Usage in Codebase

**Loading Data**:
```python
# Via pandas
import pandas as pd
df = pd.read_parquet('simulated_data/hackathon/.../episode-0-....parquet')

# Via dashboard loader
from collab_env.dashboard.utils.simulation_loader import SimulationDataLoader
loader = SimulationDataLoader()
sim_info = loader.register_simulation('sim_1', folder_path, config_path)
episode_data = loader.load_episode('sim_1', episode_id=0)
```

**Dashboard Integration**: [collab_env/dashboard/utils/simulation_loader.py:245-271](collab_env/dashboard/utils/simulation_loader.py#L245-L271)
- Converts parquet to track format with `{track_id, x, y, z, type, v_x, v_y, v_z}` per frame
- Resolves mesh paths and loads config parameters
- Provides episode browsing and playback UI

### Data Grouping

**Hierarchy**:
- **Session**: Simulation run folder (multiple episodes with same config)
- **Episode**: Single parquet file (one complete simulation trajectory)
- **Tracklet**: Time series for one agent_id across all frames
- **Properties**: Position, velocity, distances at each timestep

**Unique Identifiers**:
- Session: Folder name with timestamp
- Episode: `episode-{N}-completed-{timestamp}` filename
- Agent: `id` column (unique within episode)
- Timestep: `time` column (frame number)

---

## 2. 2D Boids Simulations

### Overview
2D boid simulations generated by `collab_env.sim.boids_gnn_temp.animal_simulation` for training Graph Neural Networks. Data stored as PyTorch datasets in `.pt` format with separate config files.

### Location
- **Data Directory**: `simulated_data/`
- **Naming Pattern**: `boid_{variant}_{config}.pt` and `boid_{variant}_{config}_config.pt`
- **Usage**: GNN training in `collab_env.gnn.interaction_particles`

### Data Structure

#### File Organization
```
simulated_data/
├── boid_single_species_basic.pt              # Trajectory data
├── boid_single_species_basic_config.pt       # Configuration
├── boid_food_basic.pt
├── boid_food_basic_config.pt
└── ...
```

#### PyTorch Dataset Structure (.pt files)

**File Format**: PyTorch serialized object (pickle-based)

**Class**: `collab_env.sim.boids_gnn_temp.animal_simulation.AnimalTrajectoryDataset`

**Dataset Structure**:
```python
dataset = torch.load('boid_single_species_basic.pt')
# Returns: AnimalTrajectoryDataset instance

len(dataset)  # Number of trajectory samples (typically 50-1000)

# Each sample is a tuple:
(positions, species) = dataset[0]

# positions: torch.Tensor, shape [timesteps, num_agents, 2]
# species: torch.Tensor, shape [num_agents], dtype int (species labels)
```

**Example Data**:
```python
# Sample from boid_single_species_basic.pt:
positions.shape  # torch.Size([10, 20, 2])
# - 10 timesteps
# - 20 agents
# - 2D coordinates (x, y)

species.shape    # torch.Size([20])
# Species labels for each agent (e.g., all 0 for single species)

# Position values are normalized to [0, 1] range
positions[0, 0, :]  # tensor([0.6626, 0.3490])  # Agent 0 at t=0
```

**Data Characteristics**:
- Positions normalized to [0, 1] coordinate space
- Velocities computed via finite differences: `v[t] = p[t+1] - p[t]`
- Accelerations: `a[t] = v[t+1] - v[t]`
- Natural timestep: dt = 1.0
- Typical sizes: 10-100 timesteps, 10-50 agents, 50-1000 samples

#### Configuration File Schema (_config.pt files)

**File Format**: PyTorch serialized dictionary

**Structure**:
```python
config = torch.load('boid_single_species_basic_config.pt')
# Returns: Dict[str, Dict[str, Any]]

# Example structure:
{
    'A': {  # Species identifier
        'visual_range': 50,          # Vision radius (pixels)
        'centering_factor': 0.005,   # Cohesion strength
        'min_distance': 15,          # Avoidance distance
        'avoid_factor': 0.05,        # Avoidance strength
        'matching_factor': 0.5,      # Alignment strength
        'margin': 5,                 # Boundary margin
        'turn_factor': 10,           # Boundary avoidance
        'speed_limit': 20,           # Max speed (pixels/frame)
        'counts': 20                 # Number of agents
    },
    'scene_size': 480.0  # Optional scene size in pixels
}
```

**Parameters by Species**:
- Each species key (e.g., 'A', 'B') contains identical parameter structure
- Multi-species configs have multiple species keys with different values
- `counts` field specifies agent population per species

### Usage in Codebase

**Loading Data**:
```python
import torch

# Load dataset
dataset = torch.load('simulated_data/boid_single_species_basic.pt',
                     weights_only=False)

# Load config
config = torch.load('simulated_data/boid_single_species_basic_config.pt',
                    weights_only=False)

# Iterate samples
for positions, species in dataset:
    # positions: [T, N, 2]
    # species: [N]
    # ... process trajectory ...
```

**GNN Training**: [collab_env/gnn/interaction_particles/run_training.py](collab_env/gnn/interaction_particles/run_training.py)
- Used as input to InteractionParticleGNN models
- Computes pairwise features (relative positions, velocities, distances)
- Predicts accelerations for learning force fields

**Analysis**: [collab_env/sim/boids_gnn_temp/analyze_dataset.py](collab_env/sim/boids_gnn_temp/analyze_dataset.py)
- Computes statistics over all samples
- Generates EDA visualizations (distributions, force fields)
- Validates data quality

### Data Grouping

**Hierarchy**:
- **Dataset**: Complete .pt file (collection of trajectories)
- **Sample**: Single trajectory (one simulation run)
- **Timestep**: Frame within trajectory
- **Agent**: Individual particle with species label

**Unique Identifiers**:
- Dataset: Filename (e.g., `boid_single_species_basic.pt`)
- Sample: Index in dataset (0 to len(dataset)-1)
- Agent: Index in positions tensor (0 to N-1)
- Timestep: Index in time dimension (0 to T-1)

**Note**: No explicit track IDs; agents identified by index position only

---

## 3. Real-World Tracked Data

### Overview
Animal tracking data from thermal/RGB video processing pipeline. Generated by `collab_env.tracking` module using YOLO/RF-DETR detection + ByteTracker tracking. Data stored as CSV files with bounding boxes or centroids.

### Location
- **Data Sources**: Google Cloud Storage buckets (`fieldwork_curated`, `fieldwork_processed`)
- **Local Processing**: Various directories as specified in session metadata
- **Formats**: `*_bboxes.csv`, `*_centroids_3d.csv`
- **Visualization**: `collab_env.dashboard` video overlay viewer

### Data Structure

#### Session Organization
```
YYYY_MM_DD-session_0001/                      # Session folder
├── Metadata.yaml                              # Session info
├── thermal_1/
│   ├── cameraInfoTime.csq                    # Raw thermal data
│   ├── cameraInfoTime_vmin-vmax.mp4          # Processed video
│   └── cameraInfoTime_vmin-vmax_bboxes.csv   # Tracking output
├── thermal_2/
│   └── ...
├── rgb_cam_1/
│   ├── cameraSerial.mp4                      # RGB video
│   └── cameraSerial_bboxes.csv               # Tracking output
└── rgb_cam_2/
    └── ...
```

#### CSV File Schema - Bounding Boxes

**File Format**: CSV (comma-separated values)

**Naming Pattern**: `*_bboxes.csv`

**Columns**:

| Column Name | Data Type | Required | Description | Example |
|------------|-----------|----------|-------------|---------|
| `track_id` | int | Yes | Unique track identifier | 0, 1, 2, ... |
| `frame` | int | Yes | Frame number (0-indexed) | 0, 1, 2, ... |
| `x1` | int/float | Yes | Top-left X coordinate (pixels) | 245 |
| `y1` | int/float | Yes | Top-left Y coordinate (pixels) | 180 |
| `x2` | int/float | Yes | Bottom-right X coordinate (pixels) | 278 |
| `y2` | int/float | Yes | Bottom-right Y coordinate (pixels) | 215 |
| `confidence` | float | Optional | Detection confidence score [0-1] | 0.87 |
| `class` | string | Optional | Object class label | "bird" |

**Alternative: Centroid Format**:

| Column Name | Data Type | Required | Description |
|------------|-----------|----------|-------------|
| `track_id` | int | Yes | Unique track identifier |
| `frame` | int | Yes | Frame number |
| `x` | int/float | Yes | Centroid X coordinate (pixels) |
| `y` | int/float | Yes | Centroid Y coordinate (pixels) |
| `confidence` | float | Optional | Detection confidence |
| `class` | string | Optional | Object class |

**Data Characteristics**:
- Coordinates in pixel space (video resolution dependent, typically 640×480 or 1920×1080)
- Frame rate: 30 fps (typical)
- Track IDs assigned by ByteTracker (may have gaps/reassignments)
- Confidence scores from YOLO/RF-DETR detector

#### CSV File Schema - 3D Centroids

**File Format**: CSV

**Naming Pattern**: `*_centroids_3d.csv`

**Columns** (inferred from usage):

| Column Name | Data Type | Description |
|------------|-----------|-------------|
| `track_id` | int | Track identifier |
| `frame` | int | Frame number |
| `x` | float | X coordinate in 3D space |
| `y` | float | Y coordinate in 3D space |
| `z` | float | Z coordinate in 3D space |
| Additional tracking-specific columns | | |

**Note**: Exact schema may vary based on tracking pipeline; typically includes reprojected 3D coordinates from multi-camera calibration.

#### Metadata File Schema (Metadata.yaml)

**File Format**: YAML

**Structure**:
```yaml
notes: "Freeform observation notes from fieldwork"

data_sources:
  - description: "thermal_1 (FLIR camera left)"
    original_path: "/local/drive/path/to/source.csq"
    path: "thermal_1/cameraInfoTime.csq"

  - description: "rgb_1 (RGB camera left)"
    original_path: "/local/drive/path/to/source.MP4"
    path: "rgb_cam_1/cameraSerial.mp4"

  - description: "thermal_2 (FLIR camera right)"
    original_path: "/local/drive/path/to/source.csq"
    path: "thermal_2/cameraInfoTime.csq"

  # ... additional sources
```

**Fields**:
- `notes`: Optional freeform text for session observations
- `data_sources`: List of source files with:
  - `description`: Camera/sensor description
  - `original_path`: Source location (provenance)
  - `path`: Relative path within session folder

### Usage in Codebase

**Tracking Pipeline**: [docs/tracking/README.md](docs/tracking/README.md)

1. **Detection**: [collab_env/tracking/model/local_model_tracking.py](collab_env/tracking/model/local_model_tracking.py)
   ```python
   from collab_env.tracking.model.local_model_tracking import (
       get_detections_from_video,
       track_objects,
       output_tracked_bboxes_csv
   )

   # Run detection
   get_detections_from_video(csv_path, video_path, output_video_path)

   # Track objects
   track_history = track_objects(detect_csv)

   # Combine detection + tracking
   output_tracked_bboxes_csv(track_csv, detect_csv, output_csv)
   ```

2. **Visualization**: Dashboard video overlay viewer
   - Auto-detects `*_bboxes.csv` files alongside videos
   - Interactive playback with track ID labels and trails
   - Handles both bbox and centroid formats

**Loading Data**:
```python
import pandas as pd

# Load tracking results
df = pd.read_csv('path/to/video_bboxes.csv')

# Access by frame
frame_data = df[df['frame'] == 42]

# Access by track
track_data = df[df['track_id'] == 5]
```

### Data Grouping

**Hierarchy**:
- **Session**: Fieldwork session folder (date + session number)
- **Video**: Individual camera recording
- **Track**: Continuous trajectory of one object (track_id)
- **Detection**: Single bounding box/centroid at one frame

**Unique Identifiers**:
- Session: Folder name `YYYY_MM_DD-session_NNNN`
- Video: Camera path (e.g., `thermal_1/cameraInfoTime.mp4`)
- Track: `track_id` (unique within video)
- Detection: `(track_id, frame)` tuple

**Temporal Information**:
- Frame-based indexing (not datetime)
- Frame rate stored in video metadata (typically 30 fps)
- Conversion to datetime requires session start time from metadata

**Coordinate Systems**:
- 2D bboxes/centroids: Pixel coordinates in video frame
- 3D centroids: World coordinates from camera calibration/triangulation

---

## 4. Summary Comparison

| Aspect | 3D Boids | 2D Boids | Real-World Tracking |
|--------|----------|----------|---------------------|
| **Format** | Parquet + YAML | PyTorch .pt | CSV + YAML |
| **Dimensions** | 3D (x, y, z) | 2D (x, y) | 2D pixels or 3D world |
| **Temporal** | Frame numbers | Frame numbers | Frame numbers |
| **Agent ID** | `id` column | Tensor index | `track_id` column |
| **Velocity** | Stored (v_x, v_y, v_z) | Computed from positions | Not stored |
| **Config** | YAML (rich) | PyTorch dict (minimal) | YAML (metadata) |
| **Sessions** | Folder = session | File = dataset | Folder = session |
| **Episodes** | Multiple per session | Multiple per dataset | One per video |
| **Typical Size** | 90K-1.2M rows/episode | 10K-50K positions/sample | Varies by video length |
| **Coordinates** | Scene units (scaled) | Normalized [0, 1] | Pixels or calibrated |
| **Use Case** | 3D simulation viz | GNN training | Real animal tracking |
| **Mesh Data** | Referenced in config | None | None (environment separate) |
| **Species/Type** | String in `type` column | Integer species labels | String in `class` column |

### Common Patterns

**All formats share**:
1. Time-series structure (frame-based indexing)
2. Multiple agents/tracks per episode/video
3. Position data as primary observable
4. Configuration/metadata in separate files
5. Grouping into sessions/datasets
6. Visualization support in dashboard

**Key Differences**:
1. **Storage**: Columnar (parquet) vs serialized (PyTorch) vs tabular (CSV)
2. **Coordinate systems**: 3D scene vs normalized 2D vs pixel space
3. **Velocity**: Explicit vs derived vs absent
4. **IDs**: Stable integer vs index vs tracker-assigned
5. **Metadata richness**: Very rich (3D boids) vs minimal (2D boids) vs moderate (tracking)

### Database Design Implications

**Unified schema should accommodate**:
- Multiple coordinate systems and dimensionalities
- Both stored and computed velocities
- Flexible ID schemes (integer, tracker-assigned, indexed)
- Rich configuration storage (YAML-compatible)
- Session/episode/tracklet hierarchy
- Optional mesh/environment references
- Species/type/class categorization
- Frame-based time with optional datetime mapping

**Storage considerations**:
- Columnar format for efficient queries (DuckDB/TimescaleDB)
- JSON/JSONB for flexible config storage
- Spatial indexing for position queries
- Time-series optimizations for temporal queries
- Efficient joins across sessions/episodes/tracklets
